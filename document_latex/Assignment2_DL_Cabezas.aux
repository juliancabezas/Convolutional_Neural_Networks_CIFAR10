\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Bishop2006}
\citation{Zhang2019}
\citation{Rosenblatt1957}
\citation{Vapnik1995}
\citation{Krizhevsky2017}
\citation{Krizhevsky2017}
\citation{Aghdam2017}
\citation{Simonyan2015}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\citation{He2016}
\citation{Shorten2019}
\citation{Liu2016}
\citation{Kolesnikov2019}
\citation{Krizhevsky2012}
\citation{Aghdam2017}
\citation{Skansi2018}
\citation{He2016}
\citation{He2016a}
\citation{He2016}
\citation{Kolesnikov2019}
\citation{Krizhevsky2017}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Background}{2}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Methods}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}CIFAR-10 Dataset}{2}{subsection.3.1}}
\citation{Simonyan2015}
\citation{Simonyan2015}
\citation{Simonyan2015}
\citation{Simonyan2015}
\citation{Skansi2018}
\citation{Hastie2009}
\citation{Simonyan2015}
\citation{He2015}
\citation{Shorten2019}
\citation{Shorten2019}
\citation{Simonyan2015}
\citation{Shorten2019}
\citation{Shorten2019}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Samples of the ten classes in CIFAR-10 dataset}}{3}{figure.1}}
\newlabel{fig:samples}{{1}{3}{Samples of the ten classes in CIFAR-10 dataset}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}VGG11 and VGG19 neural networks}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Loss function and optimization criterion}{3}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Data Augmentation techniques}{3}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}Hyperparameter tuning}{3}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces VGG11 (top) and VGG19 (bottom) architectures used in this study, the convolution layer are denoted as conv(kernel size) - (output channels), each convolution layer is followed by a ReLu activation function \cite  {Simonyan2015}}}{4}{figure.2}}
\newlabel{fig:vgg}{{2}{4}{VGG11 (top) and VGG19 (bottom) architectures used in this study, the convolution layer are denoted as conv(kernel size) - (output channels), each convolution layer is followed by a ReLu activation function \cite {Simonyan2015}}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Samples of the ten classes in CIFAR-10 dataset}}{4}{figure.3}}
\newlabel{fig:samples}{{3}{4}{Samples of the ten classes in CIFAR-10 dataset}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Code and processing}{4}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Results and discussion}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Data augmentation}{4}{subsection.5.1}}
\citation{Liu2016}
\citation{Kolesnikov2019}
\citation{Huang2018}
\citation{He2016a}
\citation{He2016a}
\citation{Liu2016}
\citation{Liu2016}
\citation{Sharma2018}
\citation{Sharma2018}
\citation{Sharma2018}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Learning rate}{5}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Testing of the final model}{5}{subsection.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Model performance in the test data}}{5}{table.1}}
\newlabel{table:test}{{1}{5}{Model performance in the test data}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance comparison with other studies}}{5}{table.2}}
\newlabel{table:testcomp}{{2}{5}{Performance comparison with other studies}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusion}{5}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training accuracy(left), Validation accuracy (centre) and Cross-entropy loss (right) of the VGG11 neural network using different data augmentation techniques. NO AUG: No augmentation, CJ: Color Jitter, RH: Random Horizontal Flip, RC: Random Crop}}{6}{figure.4}}
\newlabel{fig:vgg11-aug}{{4}{6}{Training accuracy(left), Validation accuracy (centre) and Cross-entropy loss (right) of the VGG11 neural network using different data augmentation techniques. NO AUG: No augmentation, CJ: Color Jitter, RH: Random Horizontal Flip, RC: Random Crop}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Training accuracy(left), Validation accuracy (centre) and Cross-entropy loss (right) of the VGG19 neural network using different data augmentation techniques. NO AUG: No augmentation, CJ: Color Jitter, RH: Random Horizontal Flip, RC: Random Crop}}{6}{figure.5}}
\newlabel{fig:vgg19-aug}{{5}{6}{Training accuracy(left), Validation accuracy (centre) and Cross-entropy loss (right) of the VGG19 neural network using different data augmentation techniques. NO AUG: No augmentation, CJ: Color Jitter, RH: Random Horizontal Flip, RC: Random Crop}{figure.5}{}}
\bibstyle{ieee_fullname}
\bibdata{library}
\bibcite{Aghdam2017}{1}
\bibcite{Bishop2006}{2}
\bibcite{Hastie2009}{3}
\bibcite{He2015}{4}
\bibcite{He2016}{5}
\bibcite{He2016a}{6}
\bibcite{Huang2018}{7}
\bibcite{Kolesnikov2019}{8}
\bibcite{Krizhevsky2012}{9}
\bibcite{Krizhevsky2017}{10}
\bibcite{Liu2016}{11}
\bibcite{Rosenblatt1957}{12}
\bibcite{Sharma2018}{13}
\bibcite{Shorten2019}{14}
\bibcite{Simonyan2015}{15}
\bibcite{Skansi2018}{16}
\bibcite{Vapnik1995}{17}
\bibcite{Zhang2019}{18}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Learning rate tuning on the VGG19 with Random Horizontal Flip and Random Crop model}}{7}{figure.6}}
\newlabel{fig:lr}{{6}{7}{Learning rate tuning on the VGG19 with Random Horizontal Flip and Random Crop model}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Bonus}{7}{section.7}}
